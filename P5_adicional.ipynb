{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modo duende con segmentación de fondo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gatete\\AppData\\Roaming\\Python\\Python311\\site-packages\\mediapipe\\tasks\\python\\vision\\image_segmenter.py:158: UserWarning: MessageFactory class is deprecated. Please use GetMessageClass() instead of MessageFactory.GetPrototype. MessageFactory class will be removed after 2024.\n",
      "  graph_config = self._runner.get_graph_config()\n",
      "C:\\Users\\gatete\\AppData\\Roaming\\Python\\Python311\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (100,49) (100,100) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 177\u001b[0m\n\u001b[0;32m    174\u001b[0m final_frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39madd(masked_background, masked_frame)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;66;03m# Procesamos el frame para agregar las orejas y el emoji de dinero\u001b[39;00m\n\u001b[1;32m--> 177\u001b[0m final_frame \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mface_mesh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# Mostramos el resultado final\u001b[39;00m\n\u001b[0;32m    180\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_frame\u001b[39m\u001b[38;5;124m\"\u001b[39m, final_frame)\n",
      "Cell \u001b[1;32mIn[1], line 114\u001b[0m, in \u001b[0;36mprocess_frame\u001b[1;34m(frame, face_mesh)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;66;03m# Superponemos la oreja derecha\u001b[39;00m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m):  \u001b[38;5;66;03m# Para las 3 capas (RGB)\u001b[39;00m\n\u001b[0;32m    112\u001b[0m         frame[ry:ry \u001b[38;5;241m+\u001b[39m right_ear_resized\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], rx:rx \u001b[38;5;241m+\u001b[39m right_ear_resized\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], c] \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m    113\u001b[0m             right_ear_resized[:, :, c] \u001b[38;5;241m*\u001b[39m (right_ear_resized[:, :, \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m) \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m--> 114\u001b[0m             \u001b[43mframe\u001b[49m\u001b[43m[\u001b[49m\u001b[43mry\u001b[49m\u001b[43m:\u001b[49m\u001b[43mry\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mright_ear_resized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrx\u001b[49m\u001b[43m:\u001b[49m\u001b[43mrx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mright_ear_resized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mright_ear_resized\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Comprobamos si la boca está abierta (puntos 0 y 17)\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m points_data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m17\u001b[39m \u001b[38;5;129;01min\u001b[39;00m points_data:\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (100,49) (100,100) "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe.tasks.python import BaseOptions\n",
    "\n",
    "# Inicializamos MediaPipe FaceMesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Lista de la segmentacion\n",
    "segmentation_result_list = []\n",
    "\n",
    "# Procesamos los resultados de la segmentación\n",
    "def segmentation_callback(result, output_image, timestamp_ms):\n",
    "    segmentation_result_list.append(result)\n",
    "\n",
    "# Configuramos las opciones del segmentador\n",
    "options = vision.ImageSegmenterOptions(\n",
    "    base_options=BaseOptions(model_asset_path=\"models/selfie_segmenter.tflite\"), \n",
    "    output_category_mask=True,\n",
    "    running_mode=vision.RunningMode.LIVE_STREAM, \n",
    "    result_callback=segmentation_callback\n",
    ")\n",
    "\n",
    "# Creamos el segmentador\n",
    "segmenter = vision.ImageSegmenter.create_from_options(options)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Cargar las imágenes de las orejas y los emojis de dinero\n",
    "left_ear_img = cv2.imread('images/orejaI.png', cv2.IMREAD_UNCHANGED) \n",
    "right_ear_img = cv2.imread('images/orejaD.png', cv2.IMREAD_UNCHANGED)\n",
    "money_emoji = cv2.imread('images/dinero.png', cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "def euclidean_distance(p1, p2):\n",
    "    return math.sqrt((p2[0] - p1[0]) ** 2 + (p2[1] - p1[1]) ** 2)\n",
    "\n",
    "# Lista para almacenar los emojis de dinero\n",
    "falling_emoji = []\n",
    "\n",
    "# Representamos el dinero\n",
    "class FallingEmoji:\n",
    "    def __init__(self, x, y, speed, time_to_live):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.speed = speed\n",
    "        self.time_to_live = time_to_live  # Tiempo de vida del emoji\n",
    "        self.creation_time = time.time()  # Momento en que se creó el emoji\n",
    "\n",
    "    def update(self):\n",
    "        self.y += self.speed  # Movimiento hacia abajo\n",
    "        if self.y > 480:  # Si el emoji se sale de la pantalla, lo reubicamos\n",
    "            self.y = 0\n",
    "            self.x = random.randint(0, 640)\n",
    "\n",
    "        if time.time() - self.creation_time > self.time_to_live:\n",
    "            return False  # El emoji debe eliminarse\n",
    "        return True  # El emoji sigue existiendo\n",
    "\n",
    "\n",
    "            \n",
    "# Procesamos los resultados y agregar las orejas y el emoji de dinero\n",
    "def process_frame(frame, face_mesh):\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "    points_data = {}\n",
    "\n",
    "    if results.multi_face_landmarks is not None:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            points = [234, 454, 0, 17]  # Añadir los puntos para las orejas (234 y 454) y los puntos de la boca (0 y 17)\n",
    "\n",
    "            for point in points:\n",
    "                if point < len(face_landmarks.landmark):\n",
    "                    x = face_landmarks.landmark[point].x\n",
    "                    y = face_landmarks.landmark[point].y\n",
    "                    z = face_landmarks.landmark[point].z\n",
    "\n",
    "                    x_pixel = int(x * frame.shape[1])\n",
    "                    y_pixel = int(y * frame.shape[0])\n",
    "\n",
    "                    points_data[point] = {'x': x_pixel, 'y': y_pixel, 'z': z}\n",
    "\n",
    "            # Colocar las orejas (puntos 234 y 454 para la oreja izquierda y derecha)\n",
    "            if 234 in points_data:\n",
    "                left_ear_position = points_data[234]\n",
    "                left_ear_resized = cv2.resize(left_ear_img, (100, 100))  # Aumenta el tamaño de la oreja\n",
    "                lx, ly = left_ear_position['x'], left_ear_position['y']\n",
    "\n",
    "                lx -= 80 \n",
    "                ly -= 60\n",
    "\n",
    "                # Superponemos la oreja izquierda\n",
    "                for c in range(0, 3):  # Para las 3 capas (RGB)\n",
    "                    frame[ly:ly + left_ear_resized.shape[0], lx:lx + left_ear_resized.shape[1], c] = \\\n",
    "                        left_ear_resized[:, :, c] * (left_ear_resized[:, :, 3] / 255.0) + \\\n",
    "                        frame[ly:ly + left_ear_resized.shape[0], lx:lx + left_ear_resized.shape[1], c] * (1.0 - left_ear_resized[:, :, 3] / 255.0)\n",
    "\n",
    "            if 454 in points_data:\n",
    "                right_ear_position = points_data[454]\n",
    "                right_ear_resized = cv2.resize(right_ear_img, (100, 100))  # Aumentamos el tamaño de la oreja\n",
    "                rx, ry = right_ear_position['x'], right_ear_position['y']\n",
    "\n",
    "                rx -= 15  \n",
    "                ry -= 60\n",
    "\n",
    "                # Superponemos la oreja derecha\n",
    "                for c in range(0, 3):  # Para las 3 capas (RGB)\n",
    "                    frame[ry:ry + right_ear_resized.shape[0], rx:rx + right_ear_resized.shape[1], c] = \\\n",
    "                        right_ear_resized[:, :, c] * (right_ear_resized[:, :, 3] / 255.0) + \\\n",
    "                        frame[ry:ry + right_ear_resized.shape[0], rx:rx + right_ear_resized.shape[1], c] * (1.0 - right_ear_resized[:, :, 3] / 255.0)\n",
    "\n",
    "            # Comprobamos si la boca está abierta (puntos 0 y 17)\n",
    "            if 0 in points_data and 17 in points_data:\n",
    "                mouth_distance = euclidean_distance(\n",
    "                    (points_data[0]['x'], points_data[0]['y']),\n",
    "                    (points_data[17]['x'], points_data[17]['y'])\n",
    "                )\n",
    "                threshold = 40 \n",
    "                if mouth_distance > threshold:\n",
    "                    if random.random() < 0.3:\n",
    "                        new_emoji = FallingEmoji(random.randint(0, frame.shape[1] - 80), 0, random.randint(2, 5), time_to_live=5)\n",
    "                        falling_emoji.append(new_emoji)\n",
    "                        \n",
    "            # Actualizar la posición de los emojis que caen\n",
    "            falling_emoji[:] = [emoji for emoji in falling_emoji if emoji.update()]\n",
    "            \n",
    "            # Actualizar la posición de los emojis que caen\n",
    "            for emoji in falling_emoji:\n",
    "                emoji.update()\n",
    "                # Redimensionamos la imagen del emoji\n",
    "                emoji_resized = cv2.resize(money_emoji, (80, 50))\n",
    "\n",
    "                # Superponemos el emoji sobre el frame\n",
    "                for c in range(0, 3):\n",
    "                    if emoji.y + emoji_resized.shape[0] <= frame.shape[0] and emoji.x + emoji_resized.shape[1] <= frame.shape[1]:\n",
    "                        frame[emoji.y:emoji.y + emoji_resized.shape[0], emoji.x:emoji.x + emoji_resized.shape[1], c] = \\\n",
    "                            emoji_resized[:, :, c] * (emoji_resized[:, :, 3] / 255.0) + \\\n",
    "                            frame[emoji.y:emoji.y + emoji_resized.shape[0], emoji.x:emoji.x + emoji_resized.shape[1], c] * (1.0 - emoji_resized[:, :, 3] / 255.0)\n",
    "\n",
    "    return frame\n",
    "\n",
    "# Creamos el FaceMesh y procesamos los frames\n",
    "with mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convertimos el frame a RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_rgb = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n",
    "\n",
    "        # Obtenemos los resultados del segmentador\n",
    "        segmenter.segment_async(frame_rgb, time.time_ns() // 1_000_000)\n",
    "\n",
    "        if len(segmentation_result_list) > 0:\n",
    "            segmentation_result = segmentation_result_list[0]\n",
    "            category_mask = segmentation_result.category_mask\n",
    "            category_mask_np = category_mask.numpy_view()\n",
    "\n",
    "            # Creamos la máscara binaria\n",
    "            mask = (category_mask_np > 0).astype(np.uint8) * 255\n",
    "            mask_resized = cv2.resize(mask, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "            # Reemplazamos el fondo con la máscara\n",
    "            background_image = cv2.imread(\"images/fondo.jpg\")\n",
    "            background_resized = cv2.resize(background_image, (frame.shape[1], frame.shape[0]))\n",
    "            masked_background = cv2.bitwise_and(background_resized, background_resized, mask=mask_resized)\n",
    "            masked_frame = cv2.bitwise_and(frame, frame, mask=255-mask_resized)\n",
    "            final_frame = cv2.add(masked_background, masked_frame)\n",
    "\n",
    "            # Procesamos el frame para agregar las orejas y el emoji de dinero\n",
    "            final_frame = process_frame(final_frame, face_mesh)\n",
    "\n",
    "            # Mostramos el resultado final\n",
    "            cv2.imshow(\"final_frame\", final_frame)\n",
    "\n",
    "        # Limpiamos la lista de resultados de segmentación\n",
    "        segmentation_result_list.clear()\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == 27:  # 'Esc' para salir\n",
    "            break\n",
    "\n",
    "# Liberamos la cámara y cerrar las ventanas\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VC_P5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
